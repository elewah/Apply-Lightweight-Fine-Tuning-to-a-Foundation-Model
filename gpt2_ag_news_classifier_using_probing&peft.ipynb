{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Kg7b7P6PW1a"
      },
      "source": [
        "# Project 1: Lightweight Fine-Tuning of a Foundation Model \n",
        "\n",
        "In this project, you will build a news topic classifier using the [GPT-2](https://huggingface.co/docs/transformers/en/model_doc/gpt2) model from the [Hugging Face Transformers](https://huggingface.co/transformers/) library.\n",
        "\n",
        "The dataset used for training and evaluation is the [AG News Topic Classification Dataset](https://huggingface.co/datasets/sh0416/ag_news). This dataset contains over 1 million news articles collected from more than 2,000 sources over a year. Each article is categorized into one of four topics: World, Sports, Business, or Science/Technology.\n",
        "\n",
        "By the end of this project, you will have fine-tuned a GPT-2 model for text classification and evaluated its performance on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "gc.collect()       # Python garbage collection\n",
        "torch.cuda.empty_cache()  # Free up the GPU cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmsOuwBEPW1h",
        "outputId": "3e1a3b94-344c-4f08-cdfd-8d938a2bb742"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37090f377e2e45b7bd6bcf3cc747a8b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "641cfc859ab14dad968d8c0ee226788a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "98ca9caa5446457da8dd898137c6308a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: 12000 samples\n",
            "test: 760 samples\n"
          ]
        }
      ],
      "source": [
        "# Import the datasets and transformers packages\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the train and test splits of the AG News dataset\n",
        "splits = [\"train\", \"test\"]  # Define the dataset splits to load\n",
        "ds = {split: ds for split, ds in zip(splits, load_dataset(\"ag_news\", split=splits))}  # Load the dataset and store it in a dictionary\n",
        "\n",
        "# For each split, shuffle the dataset and select a subset of samples\n",
        "for split in splits:\n",
        "    print(f\"{split}: {len(ds[split])//10} samples\")  # Print the number of samples in the subset\n",
        "    ds[split] = ds[split].shuffle(seed=42).select(range(len(ds[split]) // 10))  # Shuffle and select 10% of the dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkbunlysPW1i"
      },
      "source": [
        "## Pre-process Datasets\n",
        "\n",
        "Next, we will preprocess our datasets by converting all the text into tokens that our model can understand. You might wonder why the text isn't already tokenized. The reason is that different models use different tokenizers, and by performing tokenization during training, we maintain flexibility to adapt to the specific tokenizer required by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gti_Zb5lPW1i",
        "outputId": "d3f87d90-9300-4014-ee6c-727eeb2a0b77"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3005bdc91f724b8a8e65284d457baf2c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/12000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b3af5f17d7e944a6b731dce2e09dd246",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/760 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[43984, 75, 13410, 1582, 47557, 416, 8956, 29560, 7941, 423, 3181, 867, 11684, 290, 4736, 287, 19483, 284, 257, 17369, 11, 262, 1110, 706, 1248, 661, 3724, 287, 23171, 379, 257, 1964, 7903, 13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# # Add a padding token to the tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    \"\"\"Preprocess the imdb dataset by returning tokenized examples.\"\"\"\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\")\n",
        "\n",
        "\n",
        "tokenized_ds = {}\n",
        "for split in splits:\n",
        "    tokenized_ds[split] = ds[split].map(preprocess_function, batched=True)\n",
        "\n",
        "\n",
        "# Show the first example of the tokenized training set\n",
        "print(tokenized_ds[\"train\"][0][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rjob417tPW1j"
      },
      "source": [
        "## Load and Configure the Model\n",
        "\n",
        "Next, we will load the model and freeze most of its parameters, keeping only the classification head trainable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8osvIcqPW1k",
        "outputId": "cc11bcb7-dd14-4c14-80cb-1f8b6020e4c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 294,912 || all params: 124,737,792 || trainable%: 0.2364\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.12/site-packages/peft/tuners/lora/layer.py:1768: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"gpt2\",\n",
        "    num_labels=4,\n",
        "    #add the label2id and id2label arguments [0,1,2,3], [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "    label2id={\"World\": 0, \"Sports\": 1, \"Business\": 2, \"Sci/Tech\": 3},\n",
        "    id2label={0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"},\n",
        ")\n",
        "\n",
        "\n",
        "#use peft library to load the model\n",
        "# !pip install peft\n",
        "from peft import get_peft_model\n",
        "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
        "# model_name_or_path = \"bigscience/mt0-large\"\n",
        "# tokenizer_name_or_path = \"bigscience/mt0-large\"\n",
        "\n",
        "# Change the task type to CAUSAL_LM\n",
        "config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
        ")\n",
        "\n",
        "lora_model = get_peft_model(model, config)\n",
        "\n",
        "print(lora_model.print_trainable_parameters())\n",
        "\n",
        "# Freeze all the parameters of the base model\n",
        "# Hint: Check the documentation at https://huggingface.co/transformers/v4.2.2/training.html\n",
        "# for param in model.base_model.parameters():\n",
        "#     # freaze all the parameters\n",
        "#     param.requires_grad = False\n",
        "\n",
        "# model.classifier\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0i96CzjPPW1k",
        "outputId": "3f3b8189-5d89-4092-aa5c-c390ab359dbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT2ForSequenceClassification(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): lora.Linear(\n",
            "            (base_layer): Conv1D(nf=2304, nx=768)\n",
            "            (lora_dropout): ModuleDict(\n",
            "              (default): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (lora_A): ModuleDict(\n",
            "              (default): Linear(in_features=768, out_features=8, bias=False)\n",
            "            )\n",
            "            (lora_B): ModuleDict(\n",
            "              (default): Linear(in_features=8, out_features=2304, bias=False)\n",
            "            )\n",
            "            (lora_embedding_A): ParameterDict()\n",
            "            (lora_embedding_B): ParameterDict()\n",
            "            (lora_magnitude_vector): ModuleDict()\n",
            "          )\n",
            "          (c_proj): Conv1D(nf=768, nx=768)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D(nf=3072, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=3072)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (score): Linear(in_features=768, out_features=4, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kem1VicZPW1l"
      },
      "source": [
        "## Time to Train the Model!\n",
        "\n",
        "We're now ready to train our model! To make this process easier, we'll use the `Trainer` class from the ðŸ¤— Transformers library. This class provides a convenient high-level interface that handles most of the training logic for us.\n",
        "\n",
        "Before setting up the `Trainer`, we'll define a function to calculate the accuracy of our model, which we'll use as an evaluation metric.\n",
        "\n",
        "This is also a good moment to introduce the concept of a **Data Collator**. As explained in the Hugging Face documentation:\n",
        "\n",
        "> A data collator is an object that creates a batch from a list of dataset samples. These samples come from either the training or evaluation dataset.\n",
        "\n",
        "> In order to form proper batches, data collators might apply some preprocessing steps, such as padding the sequences to the same length.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "HF1TRtgtPW1m",
        "outputId": "589187e7-29a9-486a-9830-339009af2953"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/tmp/ipykernel_296058/3461524644.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12000' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [12000/12000 11:46:06, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.172200</td>\n",
              "      <td>1.547827</td>\n",
              "      <td>0.797368</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=12000, training_loss=1.4286685180664063, metrics={'train_runtime': 42371.599, 'train_samples_per_second': 0.283, 'train_steps_per_second': 0.283, 'total_flos': 6292978532352000.0, 'train_loss': 1.4286685180664063, 'epoch': 1.0})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
        "\n",
        "# Define a function to compute evaluation metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Compute accuracy for the evaluation predictions.\n",
        "    Args:\n",
        "        eval_pred: A tuple containing predictions and labels.\n",
        "    Returns:\n",
        "        A dictionary with the accuracy metric.\n",
        "    \"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    # Get the index of the highest probability for each prediction\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    # Calculate accuracy by comparing predictions with labels\n",
        "    return {\"accuracy\": (predictions == labels).mean()}\n",
        "\n",
        "# Set the padding token ID for the model configuration\n",
        "lora_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Initialize the HuggingFace Trainer class\n",
        "# The Trainer class simplifies the training and evaluation process for PyTorch models\n",
        "trainer = Trainer(\n",
        "    model=lora_model,  # The model to be trained\n",
        "    args=TrainingArguments(\n",
        "        output_dir=\"./data/sentiment_analysis\",  # Directory to save model checkpoints\n",
        "        learning_rate=2e-3,  # Learning rate for the optimizer\n",
        "        per_device_train_batch_size=1,  # Batch size for training\n",
        "        per_device_eval_batch_size=1,  # Batch size for evaluation\n",
        "        num_train_epochs=1,  # Number of training epochs\n",
        "        weight_decay=0.01,  # Weight decay for regularization\n",
        "        evaluation_strategy=\"epoch\",  # Evaluate the model at the end of each epoch\n",
        "        save_strategy=\"epoch\",  # Save the model at the end of each epoch\n",
        "        load_best_model_at_end=True,  # Load the best model at the end of training\n",
        "    ),\n",
        "    train_dataset=tokenized_ds[\"train\"],  # Training dataset\n",
        "    eval_dataset=tokenized_ds[\"test\"],  # Evaluation dataset\n",
        "    tokenizer=tokenizer,  # Tokenizer used for preprocessing\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),  # Data collator for padding sequences\n",
        "    compute_metrics=compute_metrics,  # Function to compute evaluation metrics\n",
        ")\n",
        "\n",
        "# Start the training process\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMzhnScqPW1n"
      },
      "source": [
        "## Evaluate the model\n",
        "\n",
        "To evaluate the model, simply call the `evaluate` method on the `trainer` object. This will test the model on the evaluation dataset and calculate the metrics defined in the `compute_metrics` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "M4o8uz1jPW1o"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 1.5478274822235107,\n",
              " 'eval_accuracy': 0.7973684210526316,\n",
              " 'eval_runtime': 699.1158,\n",
              " 'eval_samples_per_second': 1.087,\n",
              " 'eval_steps_per_second': 1.087,\n",
              " 'epoch': 1.0}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Show the performance of the model on the test set\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIM6-fYnPW1o"
      },
      "source": [
        "### View the results\n",
        "\n",
        "Let's examine two examples along with their labels and predicted values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kg1c2KRJPW1p"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>predicted_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Indian board plans own telecast of Australia s...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Stocks Higher on Drop in Jobless Claims A shar...</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label  predicted_label\n",
              "0  Indian board plans own telecast of Australia s...      1                1\n",
              "1  Stocks Higher on Drop in Jobless Claims A shar...      2                3"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert the test dataset into a pandas DataFrame\n",
        "df = pd.DataFrame(tokenized_ds[\"test\"])\n",
        "\n",
        "# Select only the \"text\" and \"label\" columns for analysis\n",
        "df = df[[\"text\", \"label\"]]\n",
        "\n",
        "# Replace HTML line breaks with spaces in the \"text\" column\n",
        "df[\"text\"] = df[\"text\"].str.replace(\"<br />\", \" \")\n",
        "\n",
        "# Use the trained model to make predictions on the test dataset\n",
        "predictions = trainer.predict(tokenized_ds[\"test\"])\n",
        "\n",
        "# Add a new column \"predicted_label\" to the DataFrame with the predicted labels\n",
        "# The predicted label is the index of the highest probability in the model's output\n",
        "df[\"predicted_label\"] = np.argmax(predictions[0], axis=1)\n",
        "\n",
        "# Display the first two rows of the DataFrame to verify the results\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QXfE5exPW1p"
      },
      "source": [
        "### Examine Incorrect Predictions\n",
        "\n",
        "Let's review some examples where the model made incorrect predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "obpB7k1xPW1q"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>predicted_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Stocks Higher on Drop in Jobless Claims A sharp drop in initial unemployment claims and bullish forecasts from Nokia and Texas Instruments sent stocks slightly higher in early trading Thursday.</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>China's inflation rate slows sharply but problems remain (AFP) AFP - China's inflation rate eased sharply in October as government efforts to cool the economy began to really bite, with food prices, one of the main culprits, showing some signs of slowing, official data showed.</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                    text  \\\n",
              "1                                                                                      Stocks Higher on Drop in Jobless Claims A sharp drop in initial unemployment claims and bullish forecasts from Nokia and Texas Instruments sent stocks slightly higher in early trading Thursday.   \n",
              "5  China's inflation rate slows sharply but problems remain (AFP) AFP - China's inflation rate eased sharply in October as government efforts to cool the economy began to really bite, with food prices, one of the main culprits, showing some signs of slowing, official data showed.   \n",
              "\n",
              "   label  predicted_label  \n",
              "1      2                3  \n",
              "5      0                2  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set the display option for pandas to show the full content of the \"text\" column without truncation\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "# Filter the DataFrame to show only the rows where the actual label (\"label\") does not match the predicted label (\"predicted_label\")\n",
        "# Display the first two rows of these mismatched predictions for analysis\n",
        "df[df[\"label\"] != df[\"predicted_label\"]].head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "#save the model\n",
        "trainer.save_model(\"models/gpt2_ag_news_peft\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
