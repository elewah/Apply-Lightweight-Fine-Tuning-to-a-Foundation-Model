{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Kg7b7P6PW1a"
      },
      "source": [
        "###  1- Train base model\n",
        "\n",
        "In this project, you will build a news topic classifier using the [GPT-2](https://huggingface.co/docs/transformers/en/model_doc/gpt2) model from the [Hugging Face Transformers](https://huggingface.co/transformers/) library.\n",
        "\n",
        "The dataset used for training and evaluation is the [AG News Topic Classification Dataset](https://huggingface.co/datasets/sh0416/ag_news). This dataset contains over 1 million news articles collected from more than 2,000 sources over a year. Each article is categorized into one of four topics: World, Sports, Business, or Science/Technology.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "gc.collect()       # Python garbage collection\n",
        "torch.cuda.empty_cache()  # Free up the GPU cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmsOuwBEPW1h",
        "outputId": "3e1a3b94-344c-4f08-cdfd-8d938a2bb742"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: 12000 samples\n",
            "test: 760 samples\n"
          ]
        }
      ],
      "source": [
        "# Import the datasets and transformers packages\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the train and test splits of the AG News dataset\n",
        "splits = [\"train\", \"test\"]  # Define the dataset splits to load\n",
        "ds = {split: ds for split, ds in zip(splits, load_dataset(\"ag_news\", split=splits))}  # Load the dataset and store it in a dictionary\n",
        "\n",
        "# For each split, shuffle the dataset and select a subset of samples\n",
        "for split in splits:\n",
        "    print(f\"{split}: {len(ds[split])//10} samples\")  # Print the number of samples in the subset\n",
        "    ds[split] = ds[split].shuffle(seed=42).select(range(len(ds[split]) // 10))  # Shuffle and select 10% of the dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkbunlysPW1i"
      },
      "source": [
        "#### Pre-process Datasets\n",
        "\n",
        "Next, we will preprocess our datasets by converting all the text into tokens that our model can understand. You might wonder why the text isn't already tokenized. The reason is that different models use different tokenizers, and by performing tokenization during training, we maintain flexibility to adapt to the specific tokenizer required by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gti_Zb5lPW1i",
        "outputId": "d3f87d90-9300-4014-ee6c-727eeb2a0b77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[43984, 75, 13410, 1582, 47557, 416, 8956, 29560, 7941, 423, 3181, 867, 11684, 290, 4736, 287, 19483, 284, 257, 17369, 11, 262, 1110, 706, 1248, 661, 3724, 287, 23171, 379, 257, 1964, 7903, 13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# # Add a padding token to the tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    \"\"\"Preprocess the imdb dataset by returning tokenized examples.\"\"\"\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\")\n",
        "\n",
        "\n",
        "tokenized_ds = {}\n",
        "for split in splits:\n",
        "    tokenized_ds[split] = ds[split].map(preprocess_function, batched=True)\n",
        "\n",
        "\n",
        "# Show the first example of the tokenized training set\n",
        "print(tokenized_ds[\"train\"][0][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rjob417tPW1j"
      },
      "source": [
        "#### Load and Configure the Model\n",
        "\n",
        "Next, we will load the model and freeze most of its parameters, keeping only the classification head trainable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8osvIcqPW1k",
        "outputId": "cc11bcb7-dd14-4c14-80cb-1f8b6020e4c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"gpt2\",\n",
        "    num_labels=4,\n",
        "    #add the label2id and id2label arguments [0,1,2,3], [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "    label2id={\"World\": 0, \"Sports\": 1, \"Business\": 2, \"Sci/Tech\": 3},\n",
        "    id2label={0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"},\n",
        ")\n",
        "\n",
        "# Freeze all the parameters of the base model\n",
        "# Hint: Check the documentation at https://huggingface.co/transformers/v4.2.2/training.html\n",
        "for param in model.base_model.parameters():\n",
        "    # freaze all the parameters\n",
        "    param.requires_grad = False\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0i96CzjPPW1k",
        "outputId": "3f3b8189-5d89-4092-aa5c-c390ab359dbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT2ForSequenceClassification(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D(nf=2304, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=768)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D(nf=3072, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=3072)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (score): Linear(in_features=768, out_features=4, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kem1VicZPW1l"
      },
      "source": [
        "#### Time to Train the Model!\n",
        "\n",
        "We're now ready to train our model! To make this process easier, we'll use the `Trainer` class from the ðŸ¤— Transformers library. This class provides a convenient high-level interface that handles most of the training logic for us.\n",
        "\n",
        "Before setting up the `Trainer`, we'll define a function to calculate the accuracy of our model, which we'll use as an evaluation metric.\n",
        "\n",
        "This is also a good moment to introduce the concept of a **Data Collator**. As explained in the Hugging Face documentation:\n",
        "\n",
        "> A data collator is an object that creates a batch from a list of dataset samples. These samples come from either the training or evaluation dataset.\n",
        "\n",
        "> In order to form proper batches, data collators might apply some preprocessing steps, such as padding the sequences to the same length.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "HF1TRtgtPW1m",
        "outputId": "589187e7-29a9-486a-9830-339009af2953"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/tmp/ipykernel_257037/3758169945.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "/opt/conda/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6000/6000 6:52:41, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.621000</td>\n",
              "      <td>0.584402</td>\n",
              "      <td>0.831579</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=6000, training_loss=0.9549507649739584, metrics={'train_runtime': 24765.93, 'train_samples_per_second': 0.485, 'train_steps_per_second': 0.242, 'total_flos': 6271235260416000.0, 'train_loss': 0.9549507649739584, 'epoch': 1.0})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
        "\n",
        "# Define a function to compute the metrics for evaluation\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred  # Unpack predictions and labels\n",
        "    predictions = np.argmax(predictions, axis=1)  # Get the index of the highest probability for each prediction\n",
        "    return {\"accuracy\": (predictions == labels).mean()}  # Calculate accuracy as the fraction of correct predictions\n",
        "\n",
        "# Set the padding token ID in the model configuration to match the tokenizer's padding token ID\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Enable gradient checkpointing to reduce memory usage during training\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# Initialize the Hugging Face Trainer class to handle the training and evaluation loop\n",
        "trainer = Trainer(\n",
        "    model=model,  # The model to be trained\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=1,  # Batch size for training (reduce if needed)\n",
        "        per_device_eval_batch_size=1,  # Batch size for evaluation (reduce if needed)\n",
        "        fp16=True,  # Enable mixed precision training\n",
        "        gradient_accumulation_steps=2,  # Accumulate gradients over multiple steps to simulate a larger batch size\n",
        "        num_train_epochs=1,  # Number of training epochs\n",
        "        weight_decay=0.01,  # Weight decay for regularization\n",
        "        evaluation_strategy=\"epoch\",  # Evaluate the model at the end of each epoch\n",
        "        save_strategy=\"epoch\",  # Save the model at the end of each epoch\n",
        "        load_best_model_at_end=True,  # Load the best model (based on evaluation) at the end of training\n",
        "    ),\n",
        "    train_dataset=tokenized_ds[\"train\"],  # Tokenized training dataset\n",
        "    eval_dataset=tokenized_ds[\"test\"],  # Tokenized evaluation dataset\n",
        "    tokenizer=tokenizer,  # Tokenizer used for preprocessing\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),  # Data collator to handle padding dynamically\n",
        "    compute_metrics=compute_metrics,  # Function to compute evaluation metrics\n",
        ")\n",
        "\n",
        "# Start the training process\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMzhnScqPW1n"
      },
      "source": [
        "#### Evaluate the model\n",
        "\n",
        "To evaluate the model, simply call the `evaluate` method on the `trainer` object. This will test the model on the evaluation dataset and calculate the metrics defined in the `compute_metrics` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "M4o8uz1jPW1o"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.5844020247459412,\n",
              " 'eval_accuracy': 0.8315789473684211,\n",
              " 'eval_runtime': 1435.8574,\n",
              " 'eval_samples_per_second': 0.529,\n",
              " 'eval_steps_per_second': 0.529,\n",
              " 'epoch': 1.0}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Show the performance of the model on the test set\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIM6-fYnPW1o"
      },
      "source": [
        "#### View the results\n",
        "\n",
        "Let's examine two examples along with their labels and predicted values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kg1c2KRJPW1p"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>predicted_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Indian board plans own telecast of Australia s...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Stocks Higher on Drop in Jobless Claims A shar...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label  predicted_label\n",
              "0  Indian board plans own telecast of Australia s...      1                0\n",
              "1  Stocks Higher on Drop in Jobless Claims A shar...      2                2"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert the test dataset into a pandas DataFrame\n",
        "df = pd.DataFrame(tokenized_ds[\"test\"])\n",
        "\n",
        "# Select only the \"text\" and \"label\" columns for analysis\n",
        "df = df[[\"text\", \"label\"]]\n",
        "\n",
        "# Replace HTML line breaks with spaces in the \"text\" column\n",
        "df[\"text\"] = df[\"text\"].str.replace(\"<br />\", \" \")\n",
        "\n",
        "# Use the trained model to make predictions on the test dataset\n",
        "predictions = trainer.predict(tokenized_ds[\"test\"])\n",
        "\n",
        "# Add a new column \"predicted_label\" to the DataFrame with the predicted labels\n",
        "# The predicted label is the index of the highest probability in the model's output\n",
        "df[\"predicted_label\"] = np.argmax(predictions[0], axis=1)\n",
        "\n",
        "# Display the first two rows of the DataFrame to verify the results\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QXfE5exPW1p"
      },
      "source": [
        "#### Examine Incorrect Predictions\n",
        "\n",
        "Let's review some examples where the model made incorrect predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "obpB7k1xPW1q"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>predicted_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Indian board plans own telecast of Australia series The Indian cricket board said on Wednesday it was making arrangements on its own to broadcast next month #39;s test series against Australia, which is under threat because of a raging TV rights dispute.</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>China's inflation rate slows sharply but problems remain (AFP) AFP - China's inflation rate eased sharply in October as government efforts to cool the economy began to really bite, with food prices, one of the main culprits, showing some signs of slowing, official data showed.</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                    text  \\\n",
              "0                         Indian board plans own telecast of Australia series The Indian cricket board said on Wednesday it was making arrangements on its own to broadcast next month #39;s test series against Australia, which is under threat because of a raging TV rights dispute.   \n",
              "5  China's inflation rate slows sharply but problems remain (AFP) AFP - China's inflation rate eased sharply in October as government efforts to cool the economy began to really bite, with food prices, one of the main culprits, showing some signs of slowing, official data showed.   \n",
              "\n",
              "   label  predicted_label  \n",
              "0      1                0  \n",
              "5      0                2  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set the display option for pandas to show the full content of the \"text\" column without truncation\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "# Filter the DataFrame to show only the rows where the actual label (\"label\") does not match the predicted label (\"predicted_label\")\n",
        "# Display the first two rows of these mismatched predictions for analysis\n",
        "df[df[\"label\"] != df[\"predicted_label\"]].head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "#save the model\n",
        "trainer.save_model(\"models/gpt2_ag_news\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### Upload the Model to Hugging Face\n",
        "\n",
        "In this step, we will upload our fine-tuned model to the Hugging Face Hub. This allows us to share the model with the community or use it in other projects. Ensure that you have your Hugging Face token set up in your environment variables for authentication.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "53dbaa1c3a024e9092390ffffb3592d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a332731a34274dbabbd6d169faa63732",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0efdd2eef4dc40bcabf50509ba76047c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "training_args.bin:   0%|          | 0.00/5.30k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/elewah/gpt2-ag-news/commit/18c304fcf760006d21176865eed7ffb24a76e266', commit_message='Upload folder using huggingface_hub', commit_description='', oid='18c304fcf760006d21176865eed7ffb24a76e266', pr_url=None, repo_url=RepoUrl('https://huggingface.co/elewah/gpt2-ag-news', endpoint='https://huggingface.co', repo_type='model', repo_id='elewah/gpt2-ag-news'), pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# !pip install huggingface_hub\n",
        "# from huggingface_hub import notebook_login\n",
        "from huggingface_hub import HfApi\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "# Load the Hugging Face token from an .env file\n",
        "\n",
        "# Load environment variables from a .env file\n",
        "load_dotenv()\n",
        "hf_token = os.getenv(\"HF_TOKEN\")\n",
        "if not hf_token:\n",
        "    raise ValueError(\"Hugging Face token not found. Please set the 'HF_TOKEN' environment variable.\")\n",
        "\n",
        "\n",
        "api = HfApi(token=hf_token)\n",
        "api.upload_folder(\n",
        "    folder_path=\"./models/gpt2_ag_news\",\n",
        "    repo_id=\"elewah/gpt2-ag-news\",\n",
        "    repo_type=\"model\",\n",
        ")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
